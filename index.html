<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Responsible Speech Generative AI.">
  <meta name="keywords" content="SpeechGen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Responsible SpeechGen</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--   <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div> -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h3 class="title is-1 publication-title">ASRU 2025 Special Session</h3>
          <h1 class="title is-1 publication-title">Responsible Speech and Audio <br> Generative AI</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
                <tr>
                    <!-- <th scope="row">TR-7</th> -->
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_akari.jpeg"></td>
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_sewon.jpeg"></td>
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_zexuan.png"></td>
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_danqi.png"></td>
                </tr>
                <tr>
                  <!-- <th scope="row">TR-7</th> -->
                  <td width="25%" style="text-align: center"><a href="https://www.cs.jhu.edu/~noa/" style="border-radius: 50%">Nicholas Andrews</a><sup>1</sup></td>
                  <td width="25%" style="text-align: center"><a href="https://xjchen.tech/" style="border-radius: 50%">Xuanjun Chen</a><sup>2</sup></td>
                  <td width="25%" style="text-align: center"><a href="https://www.linkedin.com/in/sungfeng-huang/" style="border-radius: 50%">Sung-Feng Huang</a><sup>3</sup>,</td>
                  <td width="25%" style="text-align: center"><a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" style="border-radius: 50%">Hung-yi Lee</a><sup>2</sup></td>
                </tr>
              </table>
            </span>
            <span class="author-block">
              <table>
                <tr>
                    <!-- <th scope="row">TR-7</th> -->
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_akari.jpeg"></td>
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_sewon.jpeg"></td>
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_zexuan.png"></td>
                    <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/profile_danqi.png"></td>
                </tr>
                <tr>
                  <!-- <th scope="row">TR-7</th> -->
                  <td width="25%" style="text-align: center"><a href="https://yc-li20.github.io/" style="border-radius: 50%">Yuanchao Li</a><sup>4</sup></td>
                  <td width="25%" style="text-align: center"><a href="https://takhemlata.github.io/" style="border-radius: 50%">Hemlata Tak</a><sup>5</sup></td>
                  <td width="25%" style="text-align: center"><a href="https://www.southampton.ac.uk/people/5zc9gl/doctor-jennifer-williams" style="border-radius: 50%">Jennifer Williams</a><sup>6</sup></td>
                  <td width="25%" style="text-align: center"><a href="https://hbwu-ntu.github.io/" style="border-radius: 50%">Haibin Wu</a><sup>7</sup></td>
                </tr>
              </table>
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Princeton University</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Sunday July 9 14:00 - 17:30 (EDT) @ Metropolitan West</b>
          </div>
          
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.cs.jhu.edu/~noa/">Nicholas Andrews</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xjchen.tech/">Xuanjun Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sungfeng-huang/">Sung-Feng Huang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php">Hung-yi Lee</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://yc-li20.github.io/">Yuanchao Li</a><sup>4</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://takhemlata.github.io/">Hemlata Tak</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.southampton.ac.uk/people/5zc9gl/doctor-jennifer-williams">Jennifer Williams</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="https://hbwu-ntu.github.io/">Haibin Wu</a><sup>7</sup>,
            </span>
          </div> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Johns Hopkins University,</span>
            <span class="author-block"><sup>2</sup>National Taiwan University,</span>
            <span class="author-block"><sup>3</sup>NVIDIA,</span>
            <span class="author-block"><sup>4</sup>University of Edinburgh</span>
            <br>
            <span class="author-block"><sup>6</sup>Pindrop,</span>
            <span class="author-block"><sup>6</sup>University of Southampton,</span>
            <span class="author-block"><sup>7</sup>Microsoft,</span>
          </div>

          <div class="column has-text-centered">
            <p>
              (The above organizers are sorted in alphabetical order by last name.)
            </p>
            <p>
            We are planning to organize this special session at ASRU 2025. üõ†Ô∏è ü§î
            </p>
            <p>
            Don‚Äôt miss it ‚Äî see you in beautiful Honolulu, Hawaii!  üòä
            </p>
          </div> 

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- About this special session. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this special session</h2>
        <div class="content has-text-justified">
          <p>
            The rapid evolution of speech and audio generative AI [1-3] presents both unprecedented opportunities and significant 
            ethical challenges, making this special session crucial for ensuring responsible advancement. The field is unlocking 
            powerful capabilities in synthetic speech, voice cloning, and sound effect generation, profoundly transforming domains 
            [4] like media production, accessibility, and human-computer interaction. However, this progress raises critical concerns 
            about responsibility [5,6], including bias, misuse, accountability, and the lack of transparency in generative model 
            operations. This session aims to unite the speech and audio research community in a focused exploration of these challenges.
          </p>
          <p>
            We welcome submissions that tackle critical issues in controllability [7], fairness [8], explainability [9,10], measurement [11,12], 
            and deepfake detection [13,14], including neural watermarking [15-17]. Ensuring precise controllability is vital for nuanced speech, 
            while fairness is essential to prevent harmful biases. Enhancing explainability builds trust, and robust measurement methodologies 
            are indispensable for evaluating system quality. Implementing effective deepfake detection, including robust neural watermarking 
            techniques, is imperative for safeguarding audio information and verifying authenticity. Each of these areas demands rigorous 
            attention to ensure the ethical and beneficial deployment of speech and audio generative technologies. The session will highlight 
            technical innovations, evaluation methodologies, and frameworks that contribute to safer, fairer, and more trustworthy generative systems. 
            It aims to foster broader discussions around standards, best practices, and accountability in real-world deployments of speech and 
            audio generative technologies. Ultimately, this session seeks to establish collaborative pathways and guidelines that will shape 
            the future of responsible speech and audio generative AI.
          </p>
          <p>
            This session seeks to advance state-of-the-art research on responsible speech/audio generation, focusing on controllability, fairness, 
            interpretability, and deepfake resilience. It aims to bridge communities working on generative modeling, responsible AI, and speech 
            forensics, while promoting collaboration across machine learning, ethics, human-computer interaction, and speech technology. In doing so, 
            the session supports development of tools, benchmarks, and evaluation protocols fostering transparency, robustness, and inclusiveness in 
            generative systems. By encouraging dialogue around risks, governance, and societal impact of speech/audio generation technologies, this 
            session hopes to lay the groundwork for a more responsible future in this fast-evolving field.
          </p>
        </div>
      </div>
    </div>
    <!--/ About this special session. -->

    <!--     Paper video. -->
    <!--     <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

    <!-- Topics. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Topics of Interest</h2>
    
        <div class="content has-text-justified">
          <ol>
            <li>
              <strong>Controllability and Transparency in Speech Generation</strong>
              <ul>
                <li>Techniques for controllable speech synthesis (e.g., speaker identity, emotion, prosody, style transfer, etc.)</li>
                <li>Human-in-the-loop approaches for steering or refining generated outputs</li>
                <li>Interpretability and explanation methods for speech generative models</li>
                <li>Aligning generated speech with user intent, prompts, or constraints</li>
              </ul>
            </li>
    
            <li>
              <strong>Fairness and Inclusion in Multilingual and Multi-Speaker Systems</strong>
              <ul>
                <li>Bias detection and mitigation across demographic groups (e.g., gender, accent, language variety)</li>
                <li>Fairness-aware evaluation of generative models across underrepresented communities</li>
                <li>Inclusive dataset design for training and testing speech generation models</li>
                <li>Generalization and transfer learning for responsible cross-lingual and cross-accent generation</li>
              </ul>
            </li>
    
            <li>
              <strong>Security, Misuse Prevention, and Deepfake Detection</strong>
              <ul>
                <li>Detection of synthetic or manipulated speech (e.g., deepfake forensics)</li>
                <li>Audio watermarking and provenance tracking for generative content</li>
                <li>Robustness of generative models against adversarial or malicious inputs</li>
                <li>Ethical frameworks and risk mitigation strategies for real-world deployment</li>
              </ul>
            </li>
            <li>
              <strong>Measurement and Evaluation for Responsible Speech Generation</strong>
              <ul>
                <li>Objective evaluation for speech synthesis quality and human subjective evaluation</li>
                <li>Fairness and transparency in evaluation</li>
                <li>Benchmarking across models and datasets</li>
              </ul>
            </li>
          </ol>
        </div>
      </div>
    </div>
    <!--/ Topics. -->
      
    <!-- Concurrent Work. -->
    <!--  <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
