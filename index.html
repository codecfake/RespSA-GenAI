<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="ASRU 2025 Special Session on Responsible Speech and Audio Generative AI: Focusing on ethical, controllable, fair, and secure speech generation systems.">
  <meta name="keywords" content="Responsible AI, Speech Generation, Audio Deepfake Detection, Fairness in TTS, Controllable Speech Synthesis, ASRU 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Responsible Speech and Audio Generative AI - ASRU 2025 Special Session</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            <span style="font-size: 80%">ASRU 2025 Special Session:</span><br />
            Responsible Speech and Audio Generative AI
          </h1>
          <p class="has-text-centered">Hi there üòä, we are planning to organize this special session at ASRU 2025. üõ†Ô∏è ü§î</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- About this special session -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          
          <h3 class="title is-4">Background</h3>
          <p>The rapid evolution of speech and audio generative AI [1-3] presents both unprecedented opportunities and significant ethical challenges, making this special session crucial for ensuring responsible advancement. The field is unlocking powerful capabilities in synthetic speech, voice cloning, and sound effect generation, profoundly transforming domains [4] like media production, accessibility, and human-computer interaction. However, this progress raises critical concerns about responsibility [5,6], including bias, misuse, accountability, and the lack of transparency in generative model operations. This session aims to unite the speech and audio research community in a focused exploration of these challenges.</p>
          <p>We welcome submissions that tackle critical issues in controllability [7], fairness [8], explainability [9,10], measurement [11,12], and deepfake detection [13,14], including neural watermarking [15-17]. Ensuring precise controllability is vital for nuanced speech, while fairness is essential to prevent harmful biases. Enhancing explainability builds trust, and robust measurement methodologies are indispensable for evaluating system quality. Implementing effective deepfake detection, including robust neural watermarking techniques, is imperative for safeguarding audio information and verifying authenticity. Each of these areas demands rigorous attention to ensure the ethical and beneficial deployment of speech and audio generative technologies. The session will highlight technical innovations, evaluation methodologies, and frameworks that contribute to safer, fairer, and more trustworthy generative systems. It aims to foster broader discussions around standards, best practices, and accountability in real-world deployments of speech and audio generative technologies. Ultimately, this session seeks to establish collaborative pathways and guidelines that will shape the future of responsible speech and audio generative AI.</p>
          
          <h3 class="title is-4">Objectives</h3>
          <p>This session seeks to advance state-of-the-art research on responsible speech/audio generation, focusing on controllability, fairness, interpretability, and deepfake resilience. It aims to bridge communities working on generative modeling, responsible AI, and speech forensics, while promoting collaboration across machine learning, ethics, human-computer interaction, and speech technology. In doing so, the session supports development of tools, benchmarks, and evaluation protocols fostering transparency, robustness, and inclusiveness in generative systems. By encouraging dialogue around risks, governance, and societal impact of speech/audio generation technologies, this session hopes to lay the groundwork for a more responsible future in this fast-evolving field.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Call for Papers -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Call for Papers</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Submission Instructions</h3>
            <p>We are following the same instructions provided for ASRU 2025. Please refer to the official ASRU 2025 website for detailed paper formatting and submission guidelines.</p>
    
            <h3 class="title is-4">Important Dates (Anywhere on Earth, TBD)</h3>
            <ul>
              <li><strong>Paper submissions open:</strong> March 28, 2025 (Welcome your ASRU submissions!)</li>
              <li><strong>Paper submissions due:</strong> May 28, 2025</li>
              <li><strong>Paper revision due:</strong> June 4, 2025</li>
              <li><strong>Acceptance notification:</strong> August 6, 2025</li>
              <li><strong>Camera-ready deadline:</strong> August 13, 2025</li>
              <li><strong>Workshop date:</strong> [TBA]</li>
            </ul>
            
            <h3 class="title is-4">Topics of Interest (including but not limited to)</h3>
            <ul>
              <li><strong>Controllability and Transparency in Speech Generation</strong>: Techniques for controllable synthesis, interpretability, human-in-the-loop generation, and alignment with prompts.</li>
              <li><strong>Fairness and Inclusion</strong>: Bias detection, inclusive dataset design, multilingual generalization, and fairness-aware evaluation.</li>
              <li><strong>Security and Deepfake Detection</strong>: Audio deepfake forensics, watermarking, provenance tracking, and adversarial robustness.</li>
              <li><strong>Measurement and Evaluation</strong>: Subjective/objective quality metrics, fairness in benchmarking, and transparent evaluation frameworks.</li>
            </ul>
          <p>We invite researchers and practitioners to contribute papers that explore the intersection of generative models, responsible AI, and speech/audio technologies. Let‚Äôs build a trustworthy and inclusive generative speech future‚Äîtogether!</p>
          </div>
      </div>
      </div>
    </div>
  </div>
</section> -->

  <!-- Call for Papers -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Call for Papers</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Submission Instructions</h3>
          <p>We are following the same instructions provided for ASRU 2025. Please refer to the official ASRU 2025 website for detailed paper formatting and submission guidelines.</p>

          <h3 class="title is-4">Important Dates (Anywhere on Earth, TBD)</h3>
          <ul>
            <li><strong>Paper submissions open:</strong> March 28, 2025 (Welcome your ASRU submissions!)</li>
            <li><strong>Paper submissions due:</strong> May 28, 2025</li>
            <li><strong>Paper revision due:</strong> June 4, 2025</li>
            <li><strong>Acceptance notification:</strong> August 6, 2025</li>
            <li><strong>Camera-ready deadline:</strong> August 13, 2025</li>
            <li><strong>Workshop date:</strong> [TBA]</li>
          </ul>

          <h3 class="title is-4">Topics of Interest (including but not limited to)</h3>
          <div class="content has-text-justified">
            <ol>
              <li>
                <strong>Controllability and Transparency in Speech Generation</strong>
                <ul>
                  <li>Techniques for controllable speech synthesis (e.g., speaker identity, emotion, prosody, style transfer, etc.)</li>
                  <li>Human-in-the-loop approaches for steering or refining generated outputs</li>
                  <li>Interpretability and explanation methods for speech generative models</li>
                  <li>Aligning generated speech with user intent, prompts, or constraints</li>
                </ul>
              </li>

              <li>
                <strong>Fairness and Inclusion in Multilingual and Multi-Speaker Systems</strong>
                <ul>
                  <li>Bias detection and mitigation across demographic groups (e.g., gender, accent, language variety)</li>
                  <li>Fairness-aware evaluation of generative models across underrepresented communities</li>
                  <li>Inclusive dataset design for training and testing speech generation models</li>
                  <li>Generalization and transfer learning for responsible cross-lingual and cross-accent generation</li>
                </ul>
              </li>

              <li>
                <strong>Security, Misuse Prevention, and Deepfake Detection</strong>
                <ul>
                  <li>Detection of synthetic or manipulated speech (e.g., deepfake forensics)</li>
                  <li>Audio watermarking and provenance tracking for generative content</li>
                  <li>Robustness of generative models against adversarial or malicious inputs</li>
                  <li>Ethical frameworks and risk mitigation strategies for real-world deployment</li>
                </ul>
              </li>

              <li>
                <strong>Measurement and Evaluation for Responsible Speech Generation</strong>
                <ul>
                  <li>Objective evaluation for speech synthesis quality and human subjective evaluation</li>
                  <li>Fairness and transparency in evaluation</li>
                  <li>Benchmarking across models and datasets</li>
                </ul>
              </li>
            </ol>
          </div>

          <p>We invite researchers and practitioners to contribute papers that explore the intersection of generative models, responsible AI, and speech/audio technologies. Let‚Äôs build a trustworthy and inclusive generative speech future‚Äîtogether!</p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Organizers -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Organizers</h2>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <table>
              <tr>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/andrews.png"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/xjchen.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/sfhuang.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/hylee.jpg"></td>
              </tr>
              <tr>
                <td width="25%" style="text-align: center"><a href="https://www.cs.jhu.edu/~noa/" style="border-radius: 50%">Nicholas Andrews</a><sup>1</sup></td>
                <td width="25%" style="text-align: center"><a href="https://xjchen.tech/" style="border-radius: 50%">Xuanjun Chen</a><sup>2</sup></td>
                <td width="25%" style="text-align: center"><a href="https://sungfeng-huang.github.io/" style="border-radius: 50%">Sung-Feng Huang</a><sup>3</sup></td>
                <td width="25%" style="text-align: center"><a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" style="border-radius: 50%">Hung-yi Lee</a><sup>2</sup></td>
              </tr>
            </table>
          </span>
          <span class="author-block">
            <table>
              <tr>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/tcli.jpeg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/hemlata.jpeg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/jennier.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/hbwu.jpeg"></td>
              </tr>
              <tr>
                <td width="25%" style="text-align: center"><a href="https://yc-li20.github.io/" style="border-radius: 50%">Yuanchao Li</a><sup>4</sup></td>
                <td width="25%" style="text-align: center"><a href="https://takhemlata.github.io/" style="border-radius: 50%">Hemlata Tak</a><sup>5</sup></td>
                <td width="25%" style="text-align: center"><a href="https://www.southampton.ac.uk/people/5zc9gl/doctor-jennifer-williams" style="border-radius: 50%">Jennifer Williams</a><sup>6</sup></td>
                <td width="25%" style="text-align: center"><a href="https://hbwu-ntu.github.io/" style="border-radius: 50%">Haibin Wu</a><sup>7</sup></td>
              </tr>
            </table>
          </span>
        </div>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>Johns Hopkins University,</span>
          <span class="author-block"><sup>2</sup>National Taiwan University,</span>
          <span class="author-block"><sup>3</sup>NVIDIA,</span>
          <span class="author-block"><sup>4</sup>University of Edinburgh</span>
          <br>
          <span class="author-block"><sup>5</sup>Pindrop,</span>
          <span class="author-block"><sup>6</sup>University of Southampton,</span>
          <span class="author-block"><sup>7</sup>Microsoft</span>
        </div>
        <div class="column has-text-centered">
          <p>(The above organizers are sorted in alphabetical order by last name.)</p>
          <p>Don‚Äôt miss it ‚Äî see you in beautiful Honolulu, Hawaii! üòä</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- References -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">References</h2>
    <ol>
      <li>Tan, Xu, et al. <i>"A survey on neural speech synthesis."</i> arXiv preprint arXiv:2106.15561 (2021).</li>
      <li>Li, Yinghao Aaron, et al. <i>"Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models."</i> NeurIPS (2023).</li>
      <li>Su, Yi, et al. <i>"Audio-Language Models for Audio-Centric Tasks: A survey."</i> arXiv preprint arXiv:2501.15177 (2025).</li>
      <li>Tolomei, Gabriele, et al. <i>"Prompt-to-OS (P2OS): Revolutionizing operating systems and human-computer interaction with integrated AI generative models."</i> IEEE CogMI (2023).</li>
      <li>Hutiri, Wiebke, et al. <i>"Not my voice! A taxonomy of ethical and safety harms of speech generators."</i> FAccT (2024).</li>
      <li>Mai, Kimberly T., et al. <i>"Warning: Humans cannot reliably detect speech deepfakes."</i> PLOS ONE 18.8 (2023): e0285333.</li>
      <li>Xie, Tianxin, et al. <i>"Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey."</i> arXiv preprint arXiv:2412.06602 (2024).</li>
      <li>Leschanowsky, Anna, et al. <i>"Examining the interplay between privacy and fairness for speech processing: A review and perspective."</i> Interspeech (2024).</li>
      <li>Schneider, Johannes. <i>"Explainable generative AI (GenXAI): A survey, conceptualization, and research agenda."</i> Artificial Intelligence Review 57.11 (2024): 289.</li>
      <li>Saeki, Takaaki, et al. <i>"SpeechBERTScore: Reference-aware automatic evaluation of speech generation leveraging NLP evaluation metrics."</i> Interspeech (2024).</li>
      <li>Cooper, Erica, et al. <i>"A review on subjective and objective evaluation of synthetic speech."</i> Acoustical Science and Technology (2024).</li>
      <li>Li, Yuanchao, et al. <i>"Rethinking emotion bias in music via Frechet audio distance."</i> arXiv preprint arXiv:2409.15545 (2024).</li>
      <li>Li, Menglu, et al. <i>"A Survey on Speech Deepfake Detection."</i> ACM Computing Surveys (2025).</li>
      <li>Wang, Xin, et al. <i>"ASVspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale."</i> ASVspoof 5 Workshop (2024).</li>
      <li>Ge, Wanying, et al. <i>"Proactive Detection of Speaker Identity Manipulation with Neural Watermarking."</i> 1st Workshop on GenAI Watermarking (2025).</li>
      <li>Juvela, Lauri, et al. <i>"Audio codec augmentation for robust collaborative watermarking of speech synthesis."</i> ICASSP (2025).</li>
      <li>Wen, Yizhu, et al. <i>"SoK: How Robust is Audio Watermarking in Generative AI models?."</i> arXiv preprint arXiv:2503.19176 (2025).</li>
    </ol>
  </div>
</section>

</body>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</html>

