<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="ASRU 2025 Special Session on Responsible Speech and Audio Generative AI: Focusing on ethical, controllable, fair, and secure speech generation systems.">
  <meta name="keywords" content="Responsible AI, Speech Generation, Audio Deepfake Detection, Fairness in TTS, Controllable Speech Synthesis, ASRU 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Responsible Speech and Audio Generative AI - ASRU 2025 Special Session</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            <span style="font-size: 60%">ASRU 2025 Special Session:</span><br />
            Responsible Speech and Audio Generative AI
          </h1>
          <p class="has-text-centered">Hi there üòä, we are planning to organize this special session at ASRU 2025, in beautiful Honolulu, Hawaii! üòä</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- About this special session -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          
          <h3 class="title is-4">Background</h3>
          <p>The rapid progress in speech and audio generative AI [1,2] brings exciting new opportunities, but also raises important ethical concerns, addressing responsible-use questions more critical than ever. Recent years have witnessed great developments in synthetic speech, voice cloning, singing voice synthesis, music, and sound effect generation, profoundly changing domains [3] like media production, accessibility, and human-computer interaction. However, this progress raises critical concerns about responsibility [4], including bias, misuse, accountability, and the lack of transparency in generative model operations. This session aims to bring together the speech and audio research community to address emerging challenges in responsible generative technologies. </p>
          <p>We welcome submissions that tackle critical issues in neural watermarking [5-7], controllability [8], fairness [9], explainability [10,11], and measurement [12,13]. Ensuring precise control is vital for nuanced speech, while fairness is essential to prevent harmful biases. Enhancing explainability builds trust, and robust measurement methodologies are indispensable for evaluating system quality. Each of these areas demands rigorous attention to ensure the ethical and beneficial deployment of speech, music, and audio generative technologies. The session will highlight technical innovations, evaluation methodologies, and frameworks that contribute to safer, fairer, and more trustworthy generative AI systems. It aims to foster broader discussions around standards, best practices, and accountability in real-world deployments of speech and audio generative technologies. Ultimately, this session seeks to establish collaborative pathways and guidelines that will shape the future of responsible speech and audio generative AI. </p>
          
          <h3 class="title is-4">Objectives</h3>
          <p>This session seeks to advance state-of-the-art research on responsible speech and audio generation, focusing on accountability, controllability, fairness, and interpretability. It aims to bridge communities working on generative modeling and responsible AI while promoting collaboration across machine learning, ethics, human-computer interaction, and speech technology. In doing so, the session will support the development of tools, benchmarks, and evaluation protocols, fostering transparency, robustness, and inclusiveness in generative systems for speech, singing voice, music, and general audio. By encouraging dialogue around risks, governance, and the societal impact of speech and audio generation technologies, this session hopes to lay the groundwork for a more responsible future in this fast-evolving field. </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Call for Papers -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Call for Papers</h2>
          <div class="content has-text-justified">
            <h3 class="title is-4">Submission Instructions</h3>
            <p>We are following the same instructions provided for ASRU 2025. Please refer to the official ASRU 2025 website for detailed paper formatting and submission guidelines.</p>
    
            <h3 class="title is-4">Important Dates (Anywhere on Earth, TBD)</h3>
            <ul>
              <li><strong>Paper submissions open:</strong> March 28, 2025 (Welcome your ASRU submissions!)</li>
              <li><strong>Paper submissions due:</strong> May 28, 2025</li>
              <li><strong>Paper revision due:</strong> June 4, 2025</li>
              <li><strong>Acceptance notification:</strong> August 6, 2025</li>
              <li><strong>Camera-ready deadline:</strong> August 13, 2025</li>
              <li><strong>Workshop date:</strong> [TBA]</li>
            </ul>
            
            <h3 class="title is-4">Topics of Interest (including but not limited to)</h3>
            <ul>
              <li><strong>Preventing Misuse and Ensuring Accountability in Speech and Audio Generation</strong>: Techniques for controllable synthesis, interpretability, human-in-the-loop generation, and alignment with prompts.</li>
              <li><strong>Ensuring Fairness and Inclusion in Speech and Audio Generation</strong>: Bias detection, inclusive dataset design, multilingual generalization, and fairness-aware evaluation.</li>
              <li><strong>Enabling Controllability and Transparency in Speech and Audio Generation</strong>: Audio deepfake forensics, watermarking, provenance tracking, and adversarial robustness.</li>
              <li><strong>Evaluating and Benchmarking for Trustworthy Speech and Audio Generation</strong>: Subjective/objective quality metrics, fairness in benchmarking, and transparent evaluation frameworks.</li>
            </ul>
          <p>We invite researchers and practitioners to contribute papers that explore the intersection of generative models, responsible AI, and speech/audio technologies. Let‚Äôs build a trustworthy and inclusive generative speech future‚Äîtogether!</p>
          </div>
      </div>
      </div>
    </div>
  </div>
</section> -->

  <!-- Call for Papers -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Call for Papers</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Submission Instructions</h3>
          <p>
            Please follow the official <a href="https://2025.ieeeasru.org/authors/author-instructions" target="_blank">ASRU 2025 Author Instructions</a> for paper formatting and submission. 
            When submitting, be sure to select our special session <strong>‚ÄúResponsible Speech and Audio Generative AI‚Äù</strong> to ensure your paper is considered for inclusion.
          </p>

          <p>
            This session will focus on the broader challenges beyond deepfake detection, including accountability, controllability, fairness, robustness of generative models, and corresponding evaluation methods. 
            Please note that another ASRU 2025 special session, 
            <a href="https://sites.google.com/view/deepfake-voice" target="_blank">Frontiers in Deepfake Voice Detection and Beyond</a>, 
            will focus more specifically on deepfake voice detection and related techniques.
          </p>

          <h3 class="title is-4">Important Dates</h3>
          <p>
            <em>All deadlines are Anywhere on Earth (AoE), unless otherwise specified.</em><br>
          </p>
          <ul>
            <li><strong>Paper submissions open:</strong> March 28, 2025 (Welcome your ASRU submissions!)</li>
            <li><strong>Paper submissions due:</strong> May 28, 2025</li>
            <li><strong>Paper revision due:</strong> June 4, 2025</li>
            <li><strong>Acceptance notification:</strong> August 6, 2025</li>
            <li><strong>Camera-ready deadline:</strong> August 13, 2025</li>
            <li><strong>Workshop date:</strong> [TBA]</li>
          </ul>

          <h3 class="title is-4">Topics of Interest</h3>
          <p>We welcome submissions on a broad range of topics, including but not limited to the following:</p>
          <div class="content has-text-justified">
            <ol>
              <li>
                <strong>Preventing Misuse and Ensuring Accountability in Speech and Audio Generation</strong>
                <ul>
                  <li>Robustness of generative speech and audio models against adversarial or malicious inputs</li>
                  <li>Robust watermarking techniques to resist degradation and detect intentional tampering</li>
                  <li>Evaluation framework simulating degradation and manipulation in real-world applications</li>
                </ul>
              </li>

              <li>
                <strong>Ensuring Fairness and Inclusion in Speech and Audio Generation</strong>
                <ul>
                  <li>Bias detection, mitigation, and fairness-aware evaluation across demographic, linguistic, and content-based variation</li>
                  <li>Generalization and transfer learning across language, accent, musical genre, and acoustic domains</li>
                  <li>Inclusive dataset design across diverse speakers, singing styles, musical genres, and acoustic conditions</li>
                </ul>
              </li>

              <li>
                <strong>Enabling Controllability and Transparency in Speech and Audio Generation</strong>
                <ul>
                  <li>Techniques for user-aligned controllable synthesis across speech, singing, music, and general audio domains</li>
                  <li>Approaches for human-in-the-loop steering or refining generated speech or audio</li>
                  <li>Techniques for explainable speech or audio generation</li>
                </ul>
              </li>

              <li>
                <strong>Evaluating and Benchmarking for Trustworthy Speech and Audio Generation</strong>
                <ul>
                  <li>Objective and subjective evaluation of synthesis quality, consistency and reliability across diverse conditions</li>
                  <li>Fair and transparent evaluation across speech, singing, music, and other audio generation tasks</li>
                  <li>Mitigating harmful, biased, or misleading content through ethical and risk management strategies</li>
                </ul>
              </li>
            </ol>
          </div>

          <p>We invite researchers and practitioners to contribute papers that explore the intersection of generative models, responsible AI, and speech/audio technologies. Let‚Äôs build a trustworthy and inclusive generative speech future‚Äîtogether!</p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Organizers -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Organizers</h2>
        <div class="is-size-6 publication-authors">
          <span class="author-block">
            <table>
              <tr>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/andrews.png"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/xjchen.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/sfhuang.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/hylee.jpg"></td>
              </tr>
              <tr>
                <td width="25%" style="text-align: center"><a href="https://www.cs.jhu.edu/~noa/" style="border-radius: 50%">Nicholas Andrews</a><sup>1</sup></td>
                <td width="25%" style="text-align: center"><a href="https://xjchen.tech/" style="border-radius: 50%">Xuanjun Chen</a><sup>2</sup></td>
                <td width="25%" style="text-align: center"><a href="https://sungfeng-huang.github.io/" style="border-radius: 50%">Sung-Feng Huang</a><sup>3</sup></td>
                <td width="25%" style="text-align: center"><a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" style="border-radius: 50%">Hung-yi Lee</a><sup>2</sup></td>
              </tr>
            </table>
          </span>
          <span class="author-block">
            <table>
              <tr>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/ycli.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/hemlata.jpeg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/jennier.jpg"></td>
                <td width="25%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/author/hbwu.jpeg"></td>
              </tr>
              <tr>
                <td width="25%" style="text-align: center"><a href="https://yc-li20.github.io/" style="border-radius: 50%">Yuanchao Li</a><sup>4</sup></td>
                <td width="25%" style="text-align: center"><a href="https://takhemlata.github.io/" style="border-radius: 50%">Hemlata Tak</a><sup>5</sup></td>
                <td width="25%" style="text-align: center"><a href="https://www.southampton.ac.uk/people/5zc9gl/doctor-jennifer-williams" style="border-radius: 50%">Jennifer Williams</a><sup>6</sup></td>
                <td width="25%" style="text-align: center"><a href="https://hbwu-ntu.github.io/" style="border-radius: 50%">Haibin Wu</a><sup>7</sup></td>
              </tr>
            </table>
          </span>
        </div>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>Johns Hopkins University,</span>
          <span class="author-block"><sup>2</sup>National Taiwan University,</span>
          <span class="author-block"><sup>3</sup>NVIDIA,</span>
          <span class="author-block"><sup>4</sup>University of Edinburgh,</span>
          <span class="author-block"><sup>5</sup>Pindrop,</span>
          <span class="author-block"><sup>6</sup>University of Southampton,</span>
          <span class="author-block"><sup>7</sup>Microsoft</span>
        </div>
        <div class="column has-text-centered">
          <p>(The above organizers are sorted in alphabetical order by last name.)</p>
          <br>
          <p>Don‚Äôt miss it ‚Äî see you in beautiful Honolulu, Hawaii! üòä</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- References -->
<!-- <section class="section">
  <div class="container is-max-desktop content"> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">References</h2>
        <div class="content has-text-justified">
          <ol>
            <li>X. Tan, et al. <i>"A survey on neural speech synthesis."</i> arXiv preprint arXiv:2106.15561 (2021).</li>
            <li>Y. Su, et al. <i>"Audio-Language Models for Audio-Centric Tasks: A survey."</i> arXiv preprint arXiv:2501.15177 (2025).</li>
            <li>G. Tolomei, et al. <i>"Prompt-to-OS: revolutionizing operating systems & human-computer interaction with integrated AI generative models."</i> in CogMI 2023.</li>
            <li>W. Hutiri, et al. <i>"Not my voice! A taxonomy of ethical and safety harms of speech generators."</i> in ACM FAccT 2024.</li>
            <li>W. Ge, et al. <i>"Proactive Detection of Speaker Identity Manipulation with Neural Watermarking."</i> in 1st Workshop on GenAI Watermarking. </li>
            <li>L. Juvela,  et al. <i>"Audio codec augmentation for robust collaborative watermarking of speech synthesis."</i> in ICASSP, 2025.</li>
            <li>Y. Wen, et al. <i>"SoK: How Robust is Audio Watermarking in Generative AI models?"</i> arXiv preprint arXiv:2503.19176 (2025). </li>
            <li>T. Xie, et al. <i>"Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey."</i> arXiv preprint arXiv:2412.06602 (2024). </li>
            <li>A. Leschanowsky, et al. <i>"Examining the interplay between privacy and fairness for speech processing: A review and perspective."</i> in Interspeech, 2024. </li>
            <li>J. Schneider. <i>"Explainable generative ai (Genxai): A survey, conceptualization, and research agenda."</i> Artificial Intelligence Review 57.11 (2024): 289. </li>
            <li>T. Saeki, et al. <i>"SpeechBERTScore: Reference-aware automatic evaluation of speech generation leveraging nlp evaluation metrics."</i> in Interspeech, 2024. </li>
            <li>E. Cooper, et al. <i>"A review on subjective and objective evaluation of synthetic speech."</i> Acoustical Science and Technology  (2024). </li>
            <li>Y. Li, et al. <i>"Rethinking emotion bias in music via Frechet audio distance."</i> in IEEE ICME (2025). </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Acknowledgment -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Acknowledgment</h2>
        <div class="content has-text-justified">
          <p>
            We would like to thank <a href="https://researchmap.jp/wangxin?lang=en" target="_blank">Prof. Xin Wang</a> and 
            <a href="https://researchmap.jp/read0205283" target="_blank">Prof. Junichi Yamagishi</a> for their valuable comments on neural watermarking.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

</body>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</html>

